{"cells":[{"cell_type":"markdown","metadata":{"id":"_2UMkJkMdcmp"},"source":["# A Quick Introduction to Optuna (https://optuna.org/)\n","\n","This Jupyter notebook goes through the basic usage of Optuna.\n","\n","- Install Optuna\n","- Write a training algorithm that involves hyperparameters\n","  - Read train/valid data\n","  - Define and train model\n","  - Evaluate model\n","- Use Optuna to tune the hyperparameters (hyperparameter optimization, HPO)\n","- Visualize HPO"]},{"cell_type":"markdown","metadata":{"id":"W8_Cm49vdcms"},"source":["## Install `optuna`\n","\n","Optuna can be installed via `pip` or `conda`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mQYTei5rdcmt"},"outputs":[],"source":["!pip install --quiet optuna"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"anXEUI0-dcmt"},"outputs":[],"source":["import optuna\n","\n","optuna.__version__"]},{"cell_type":"markdown","metadata":{"id":"8fKmFmWcdcmu"},"source":["## Optimize Hyperparameters\n","\n","### Define a simple scikit-learn model\n","\n","We start with a simple random forest model to classify flowers in the Iris dataset. We define a function called `objective` that encapsulates the whole training process and outputs the accuracy of the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YAqmXF0Edcmu"},"outputs":[],"source":["import sklearn.datasets\n","import sklearn.ensemble\n","import sklearn.model_selection\n","\n","def objective():\n","    iris = sklearn.datasets.load_iris()  # Prepare the data.\n","    \n","    # Decision Tree를 여러 개 쌓은 RandomForest에서 중요한 hyper parameter?\n","    # number & depth of decision tree\n","    clf = sklearn.ensemble.RandomForestClassifier(    \n","        n_estimators=5, max_depth=3)  # Define the model.\n","    \n","    # cv : number of cross validation\n","    return sklearn.model_selection.cross_val_score(\n","        clf, iris.data, iris.target, n_jobs=-1, cv=3).mean()  # Train and evaluate the model.\n","\n","print('Accuracy: {}'.format(objective()))"]},{"cell_type":"markdown","metadata":{"id":"mdYfX1ewdcmv"},"source":["### Optimize hyperparameters of the model\n","\n","The hyperparameters of the above algorithm are `n_estimators` and `max_depth` for which we can try different values to see if the model accuracy can be improved. The `objective` function is modified to accept a trial object. This trial has several methods for sampling hyperparameters. We create a study to run the hyperparameter optimization and finally read the best hyperparameters."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J1WTXfPxdcmv"},"outputs":[],"source":["import optuna\n","\n","def objective(trial):\n","    iris = sklearn.datasets.load_iris()\n","    \n","    # define search space\n","    n_estimators = trial.suggest_int('n_estimators', 2, 20) # 자연수\n","    max_depth = int(trial.suggest_float('max_depth', 1, 32, log=True)) # 자연수\n","    \n","    clf = sklearn.ensemble.RandomForestClassifier(\n","        n_estimators=n_estimators, max_depth=max_depth)\n","    \n","    return sklearn.model_selection.cross_val_score(\n","        clf, iris.data, iris.target, n_jobs=-1, cv=3).mean()\n","\n","study = optuna.create_study(direction='maximize') # maximize accuracy, minimize loss\n","study.optimize(objective, n_trials=100) # if num of epochs is 20, 20*100 trials are performed\n","\n","trial = study.best_trial\n","\n","print('Accuracy: {}'.format(trial.value))\n","print(\"Best hyperparameters: {}\".format(trial.params))"]},{"cell_type":"markdown","metadata":{"id":"Iyw2Ws_v_Xww"},"source":["# Optuna <=> Bayesian Optimization\n","\n","https://distill.pub/2020/bayesian-optimization/"]},{"cell_type":"markdown","metadata":{"id":"CcO7UTKzdcmw"},"source":["It is possible to condition hyperparameters using Python `if` statements. We can for instance include another classifier, a support vector machine, in our HPO and define hyperparameters specific to the random forest model and the support vector machine."]},{"cell_type":"markdown","metadata":{},"source":["bayesian optimization: uncertainty 측정 -> uncertainty 낮으면서 성능이 높은 곳 vs uncertainty 높지만 새로운 가능성이 있는 곳 -> search 갯수가 적더라도 효과적으로 찾아준다  \n","                      -> 따라서, hyper parameter optimization에 쓰인다. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nR7Zwd2-dcmw"},"outputs":[],"source":["# model selection is also hyper parameter ex) RandomForest vs SVM vs Neural Network\n","import sklearn.svm\n","\n","def objective(trial):\n","    iris = sklearn.datasets.load_iris()\n","\n","    classifier = trial.suggest_categorical('classifier', ['RandomForest', 'SVC']) # SVC: SVM Classifier\n","    \n","    if classifier == 'RandomForest':\n","        # search hyper parameter - int\n","        n_estimators = trial.suggest_int('n_estimators', 2, 20) \n","        max_depth = int(trial.suggest_float('max_depth', 1, 32, log=True))\n","\n","        # build classifier\n","        clf = sklearn.ensemble.RandomForestClassifier(\n","            n_estimators=n_estimators, max_depth=max_depth)\n","    else:\n","        # search hyper parameter - float\n","        c = trial.suggest_float('svc_c', 1e-10, 1e10, log=True)\n","        \n","        # build classifier\n","        clf = sklearn.svm.SVC(C=c, gamma='auto')\n","\n","    return sklearn.model_selection.cross_val_score(\n","        clf, iris.data, iris.target, n_jobs=-1, cv=3).mean()\n","\n","study = optuna.create_study(direction='maximize') # maximize accuracy\n","study.optimize(objective, n_trials=100)\n","\n","trial = study.best_trial\n","\n","print('Accuracy: {}'.format(trial.value))\n","print(\"Best hyperparameters: {}\".format(trial.params))\n","# SVC의 성능이 RandomForest에 비해 압도적으로 높기 떄문에, trials의 뒤로 갈수록 SVC 위주로 searching"]},{"cell_type":"markdown","metadata":{"id":"CvpgImrFdcmx"},"source":["### Plotting the study\n","\n","Plotting the optimization history of the study."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kRzObk3Hdcmx"},"outputs":[],"source":["# trials의 앞쪽일수록 성능이 낮은 점이 많다 -> uncertainty가 높아 random search를 하면서 실패한 일이 많은 것\n","# trials의 뒷쪽일수록 uncertainty가 대부분 낮아진 상태이므로 실패를 거의 하지 않았다\n","optuna.visualization.plot_optimization_history(study)"]},{"cell_type":"markdown","metadata":{"id":"evl41Y5Jdcmy"},"source":["Plotting the accuracies for each hyperparameter for each trial."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eHoQedu3dcmy"},"outputs":[],"source":["# SVM이 RandomForest보다 성능이 높다\n","optuna.visualization.plot_slice(study)"]},{"cell_type":"markdown","metadata":{"id":"bfuoDFu5dcmy"},"source":["Plotting the accuracy surface for the hyperparameters involved in the random forest model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-UYfczygdcmy"},"outputs":[],"source":["# loss curve, contour plot -> 최저점에서 최대점으로 갈수록 성능이 좋아진다(색깔이 진해짐)\n","# recent trend: visualizing the loss landscape of neural nets\n","# smoother loss landscape = less local minimum = stable learning\n","optuna.visualization.plot_contour(study, params=['n_estimators', 'max_depth'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0c9vrHI32XXl"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/optuna/optuna-examples/blob/main/quickstart.ipynb","timestamp":1666633057503}]},"kernelspec":{"display_name":"Python 3.6.15 ('labelme')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.15"},"vscode":{"interpreter":{"hash":"e9ba6573f01eee0d376cf2f0a919127db08d0092cf0d76ab8ca056e47b52c42e"}}},"nbformat":4,"nbformat_minor":0}
