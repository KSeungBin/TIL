{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1JrjNO8UlI7"
      },
      "source": [
        "## Tutorial Code for MAML and ProtoTypical Network\n",
        "- Most of the codes are from examples from [higher](https://github.com/facebookresearch/higher/blob/main/examples/support/omniglot_loaders.py).\n",
        "- If there is a bug, please contact lsnfamily02@kaist.ac.kr (이신의)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AhLyvlL5snT",
        "outputId": "9a27af1e-165b-4de2-b6ec-ca70429d96d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: higher in /usr/local/lib/python3.7/dist-packages (0.2.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from higher) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->higher) (4.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install higher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fqza5Kgs64K_"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import time\n",
        "import typing\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "mpl.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('bmh')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import higher\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "import torch.utils.data as data\n",
        "import os\n",
        "import errno\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExtulGcJVIhk"
      },
      "source": [
        "## Dataset and DataLoader for Omniglot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_se_Dmf76Agk"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "# These Omniglot loaders are from Jackie Loong's PyTorch MAML implementation:\n",
        "#     https://github.com/dragen1860/MAML-Pytorch\n",
        "#     https://github.com/dragen1860/MAML-Pytorch/blob/master/omniglot.py\n",
        "#     https://github.com/dragen1860/MAML-Pytorch/blob/master/omniglotNShot.py\n",
        "\n",
        "import  torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import os\n",
        "import errno\n",
        "\n",
        "\n",
        "class Omniglot(data.Dataset):\n",
        "    urls = [\n",
        "        'https://github.com/brendenlake/omniglot/raw/master/python/images_background.zip',\n",
        "        'https://github.com/brendenlake/omniglot/raw/master/python/images_evaluation.zip'\n",
        "    ]\n",
        "    raw_folder = 'raw'\n",
        "    processed_folder = 'processed'\n",
        "    training_file = 'training.pt'\n",
        "    test_file = 'test.pt'\n",
        "\n",
        "    '''\n",
        "    The items are (filename,category). The index of all the categories can be found in self.idx_classes\n",
        "    Args:\n",
        "    - root: the directory where the dataset will be stored\n",
        "    - transform: how to transform the input\n",
        "    - target_transform: how to transform the target\n",
        "    - download: need to download the dataset\n",
        "    '''\n",
        "\n",
        "    def __init__(self, root, transform=None, target_transform=None,\n",
        "                 download=False):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        if not self._check_exists():\n",
        "            if download:\n",
        "                self.download()\n",
        "            else:\n",
        "                raise RuntimeError('Dataset not found.' + ' You can use download=True to download it')\n",
        "\n",
        "        self.all_items = find_classes(os.path.join(self.root, self.processed_folder))\n",
        "        self.idx_classes = index_classes(self.all_items)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        filename = self.all_items[index][0]\n",
        "        img = str.join('/', [self.all_items[index][2], filename])\n",
        "\n",
        "        target = self.idx_classes[self.all_items[index][1]]\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_items)\n",
        "\n",
        "    def _check_exists(self):\n",
        "        return os.path.exists(os.path.join(self.root, self.processed_folder, \"images_evaluation\")) and \\\n",
        "               os.path.exists(os.path.join(self.root, self.processed_folder, \"images_background\"))\n",
        "\n",
        "    def download(self):\n",
        "        from six.moves import urllib\n",
        "        import zipfile\n",
        "\n",
        "        if self._check_exists():\n",
        "            return\n",
        "\n",
        "        # download files\n",
        "        try:\n",
        "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
        "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
        "        except OSError as e:\n",
        "            if e.errno == errno.EEXIST:\n",
        "                pass\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "        for url in self.urls:\n",
        "            print('== Downloading ' + url)\n",
        "            data = urllib.request.urlopen(url)\n",
        "            filename = url.rpartition('/')[2]\n",
        "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
        "            with open(file_path, 'wb') as f:\n",
        "                f.write(data.read())\n",
        "            file_processed = os.path.join(self.root, self.processed_folder)\n",
        "            print(\"== Unzip from \" + file_path + \" to \" + file_processed)\n",
        "            zip_ref = zipfile.ZipFile(file_path, 'r')\n",
        "            zip_ref.extractall(file_processed)\n",
        "            zip_ref.close()\n",
        "        print(\"Download finished.\")\n",
        "\n",
        "\n",
        "def find_classes(root_dir):\n",
        "    retour = []\n",
        "    for (root, dirs, files) in os.walk(root_dir):\n",
        "        for f in files:\n",
        "            if (f.endswith(\"png\")):\n",
        "                r = root.split('/')\n",
        "                lr = len(r)\n",
        "                retour.append((f, r[lr - 2] + \"/\" + r[lr - 1], root))\n",
        "    print(\"== Found %d items \" % len(retour))\n",
        "    return retour\n",
        "\n",
        "\n",
        "def index_classes(items):\n",
        "    idx = {}\n",
        "    for i in items:\n",
        "        if i[1] not in idx:\n",
        "            idx[i[1]] = len(idx)\n",
        "    print(\"== Found %d classes\" % len(idx))\n",
        "    return idx\n",
        "\n",
        "\n",
        "class OmniglotNShot(object):\n",
        "\n",
        "    def __init__(self, root, batchsz, n_way, k_shot, k_query, imgsz, device=None):\n",
        "        \"\"\"\n",
        "        Different from mnistNShot, the\n",
        "        :param root:\n",
        "        :param batchsz: task num\n",
        "        :param n_way:\n",
        "        :param k_shot:\n",
        "        :param k_qry:\n",
        "        :param imgsz:\n",
        "        \"\"\"\n",
        "\n",
        "        self.resize = imgsz\n",
        "        self.device = device\n",
        "        if not os.path.isfile(os.path.join(root, 'omniglot.npy')):\n",
        "            # if root/data.npy does not exist, just download it\n",
        "            self.x = Omniglot(\n",
        "                root, download=True,\n",
        "                transform=transforms.Compose(\n",
        "                    [lambda x: Image.open(x).convert('L'),\n",
        "                     lambda x: x.resize((imgsz, imgsz)),\n",
        "                     lambda x: np.reshape(x, (imgsz, imgsz, 1)),\n",
        "                     lambda x: np.transpose(x, [2, 0, 1]),\n",
        "                     lambda x: x/255.]),\n",
        "            )\n",
        "\n",
        "            temp = dict()  # {label:img1, img2..., 20 imgs, label2: img1, img2,... in total, 1623 label}\n",
        "            for (img, label) in self.x:\n",
        "                if label in temp.keys():\n",
        "                    temp[label].append(img)\n",
        "                else:\n",
        "                    temp[label] = [img]\n",
        "\n",
        "            self.x = []\n",
        "            for label, imgs in temp.items():  # labels info deserted , each label contains 20imgs\n",
        "                self.x.append(np.array(imgs))\n",
        "\n",
        "            # as different class may have different number of imgs\n",
        "            self.x = np.array(self.x).astype(float)  # [[20 imgs],..., 1623 classes in total]\n",
        "            # each character contains 20 imgs\n",
        "            print('data shape:', self.x.shape)  # [1623, 20, 84, 84, 1]\n",
        "            temp = []  # Free memory\n",
        "            # save all dataset into npy file.\n",
        "            np.save(os.path.join(root, 'omniglot.npy'), self.x)\n",
        "            print('write into omniglot.npy.')\n",
        "        else:\n",
        "            # if data.npy exists, just load it.\n",
        "            self.x = np.load(os.path.join(root, 'omniglot.npy'))\n",
        "            print('load from omniglot.npy.')\n",
        "\n",
        "        # [1623, 20, 84, 84, 1]\n",
        "        # TODO: can not shuffle here, we must keep training and test set distinct!\n",
        "        self.x_train, self.x_test = self.x[:1200], self.x[1200:]\n",
        "\n",
        "        # self.normalization()\n",
        "\n",
        "        self.batchsz = batchsz\n",
        "        self.n_cls = self.x.shape[0]  # 1623\n",
        "        self.n_way = n_way  # n way\n",
        "        self.k_shot = k_shot  # k shot\n",
        "        self.k_query = k_query  # k query\n",
        "        assert (k_shot + k_query) <=20\n",
        "\n",
        "        # save pointer of current read batch in total cache\n",
        "        self.indexes = {\"train\": 0, \"test\": 0}\n",
        "        self.datasets = {\"train\": self.x_train, \"test\": self.x_test}  # original data cached\n",
        "        print(\"DB: train\", self.x_train.shape, \"test\", self.x_test.shape)\n",
        "\n",
        "        self.datasets_cache = {\"train\": self.load_data_cache(self.datasets[\"train\"]),  # current epoch data cached\n",
        "                               \"test\": self.load_data_cache(self.datasets[\"test\"])}\n",
        "\n",
        "    def normalization(self):\n",
        "        \"\"\"\n",
        "        Normalizes our data, to have a mean of 0 and sdt of 1\n",
        "        \"\"\"\n",
        "        self.mean = np.mean(self.x_train)\n",
        "        self.std = np.std(self.x_train)\n",
        "        self.max = np.max(self.x_train)\n",
        "        self.min = np.min(self.x_train)\n",
        "        # print(\"before norm:\", \"mean\", self.mean, \"max\", self.max, \"min\", self.min, \"std\", self.std)\n",
        "        self.x_train = (self.x_train - self.mean) / self.std\n",
        "        self.x_test = (self.x_test - self.mean) / self.std\n",
        "\n",
        "        self.mean = np.mean(self.x_train)\n",
        "        self.std = np.std(self.x_train)\n",
        "        self.max = np.max(self.x_train)\n",
        "        self.min = np.min(self.x_train)\n",
        "\n",
        "    # print(\"after norm:\", \"mean\", self.mean, \"max\", self.max, \"min\", self.min, \"std\", self.std)\n",
        "\n",
        "    def load_data_cache(self, data_pack):\n",
        "        \"\"\"\n",
        "        Collects several batches data for N-shot learning\n",
        "        :param data_pack: [cls_num, 20, 84, 84, 1]\n",
        "        :return: A list with [support_set_x, support_set_y, target_x, target_y] ready to be fed to our networks\n",
        "        \"\"\"\n",
        "        #  take 5 way 1 shot as example: 5 * 1\n",
        "        setsz = self.k_shot * self.n_way\n",
        "        querysz = self.k_query * self.n_way\n",
        "        data_cache = []\n",
        "\n",
        "        # print('preload next 50 caches of batchsz of batch.')\n",
        "        for sample in range(10):  # num of episodes\n",
        "\n",
        "            x_spts, y_spts, x_qrys, y_qrys = [], [], [], []\n",
        "            for i in range(self.batchsz):  # one batch means one set\n",
        "\n",
        "                x_spt, y_spt, x_qry, y_qry = [], [], [], []\n",
        "                selected_cls = np.random.choice(data_pack.shape[0], self.n_way, False)\n",
        "\n",
        "                for j, cur_class in enumerate(selected_cls):\n",
        "\n",
        "                    selected_img = np.random.choice(20, self.k_shot + self.k_query, False)\n",
        "\n",
        "                    # meta-training and meta-test\n",
        "                    x_spt.append(data_pack[cur_class][selected_img[:self.k_shot]])\n",
        "                    x_qry.append(data_pack[cur_class][selected_img[self.k_shot:]])\n",
        "                    y_spt.append([j for _ in range(self.k_shot)])\n",
        "                    y_qry.append([j for _ in range(self.k_query)])\n",
        "\n",
        "                # shuffle inside a batch\n",
        "                perm = np.random.permutation(self.n_way * self.k_shot)\n",
        "                x_spt = np.array(x_spt).reshape(self.n_way * self.k_shot, 1, self.resize, self.resize)[perm]\n",
        "                y_spt = np.array(y_spt).reshape(self.n_way * self.k_shot)[perm]\n",
        "                perm = np.random.permutation(self.n_way * self.k_query)\n",
        "                x_qry = np.array(x_qry).reshape(self.n_way * self.k_query, 1, self.resize, self.resize)[perm]\n",
        "                y_qry = np.array(y_qry).reshape(self.n_way * self.k_query)[perm]\n",
        "\n",
        "                # append [sptsz, 1, 84, 84] => [b, setsz, 1, 84, 84]\n",
        "                x_spts.append(x_spt)\n",
        "                y_spts.append(y_spt)\n",
        "                x_qrys.append(x_qry)\n",
        "                y_qrys.append(y_qry)\n",
        "\n",
        "\n",
        "            # [b, setsz, 1, 84, 84]\n",
        "            x_spts = np.array(x_spts).astype(float).reshape(self.batchsz, setsz, 1, self.resize, self.resize)\n",
        "            y_spts = np.array(y_spts).astype(int).reshape(self.batchsz, setsz)\n",
        "            # [b, qrysz, 1, 84, 84]\n",
        "            x_qrys = np.array(x_qrys).astype(float).reshape(self.batchsz, querysz, 1, self.resize, self.resize)\n",
        "            y_qrys = np.array(y_qrys).astype(int).reshape(self.batchsz, querysz)\n",
        "\n",
        "            x_spts, y_spts, x_qrys, y_qrys = [\n",
        "                torch.from_numpy(z).to(self.device) for z in\n",
        "                [x_spts, y_spts, x_qrys, y_qrys]\n",
        "            ]\n",
        "            # convert double to float\n",
        "            x_spts = x_spts.float()\n",
        "            x_qrys = x_qrys.float()\n",
        "            data_cache.append([x_spts, y_spts, x_qrys, y_qrys])\n",
        "\n",
        "        return data_cache\n",
        "\n",
        "    def next(self, mode='train'):\n",
        "        \"\"\"\n",
        "        Gets next batch from the dataset with name.\n",
        "        :param mode: The name of the splitting (one of \"train\", \"val\", \"test\")\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # update cache if indexes is larger cached num\n",
        "        if self.indexes[mode] >= len(self.datasets_cache[mode]):\n",
        "            self.indexes[mode] = 0\n",
        "            self.datasets_cache[mode] = self.load_data_cache(self.datasets[mode])\n",
        "\n",
        "        next_batch = self.datasets_cache[mode][self.indexes[mode]]\n",
        "        self.indexes[mode] += 1\n",
        "\n",
        "        return next_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nbRa38DVRrB"
      },
      "source": [
        "## MAML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jD29srn25yC9"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "#\n",
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"\n",
        "This example shows how to use higher to do Model Agnostic Meta Learning (MAML)\n",
        "for few-shot Omniglot classification.\n",
        "For more details see the original MAML paper:\n",
        "https://arxiv.org/abs/1703.03400\n",
        "This code has been modified from Jackie Loong's PyTorch MAML implementation:\n",
        "https://github.com/dragen1860/MAML-Pytorch/blob/master/omniglot_train.py\n",
        "Our MAML++ fork and experiments are available at:\n",
        "https://github.com/bamos/HowToTrainYourMAMLPytorch\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def run_maml():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--n_way', type=int, help='n way', default=5)\n",
        "    parser.add_argument(\n",
        "        '--k_spt', type=int, help='k shot for support set', default=5)\n",
        "    parser.add_argument(\n",
        "        '--k_qry', type=int, help='k shot for query set', default=15)\n",
        "    parser.add_argument(\n",
        "        '--task_num',\n",
        "        type=int,\n",
        "        help='meta batch size, namely task num',\n",
        "        default=32)\n",
        "    parser.add_argument('--seed', type=int, help='random seed', default=1004)\n",
        "    args = parser.parse_args(args=[])\n",
        "\n",
        "    torch.manual_seed(args.seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "\n",
        "    # Set up the Omniglot loader.\n",
        "    device = torch.cuda.current_device()\n",
        "    db = OmniglotNShot(\n",
        "        './tmp/omniglot-data',\n",
        "        batchsz=args.task_num,\n",
        "        n_way=args.n_way,\n",
        "        k_shot=args.k_spt,\n",
        "        k_query=args.k_qry,\n",
        "        imgsz=28,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    # Create a vanilla PyTorch neural network that will be\n",
        "    # automatically monkey-patched by higher later.\n",
        "    # Before higher, models could *not* be created like this\n",
        "    # and the parameters needed to be manually updated and copied\n",
        "    # for the updates.\n",
        "    net = nn.Sequential(\n",
        "        nn.Conv2d(1, 64, 3),\n",
        "        nn.BatchNorm2d(64, momentum=1, affine=True),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(2, 2),\n",
        "        nn.Conv2d(64, 64, 3),\n",
        "        nn.BatchNorm2d(64, momentum=1, affine=True),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(2, 2),\n",
        "        nn.Conv2d(64, 64, 3),\n",
        "        nn.BatchNorm2d(64, momentum=1, affine=True),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(2, 2),\n",
        "        nn.Flatten(start_dim=1),\n",
        "        nn.Linear(64, args.n_way)).to(device)\n",
        "\n",
        "    # We will use Adam to (meta-)optimize the initial parameters\n",
        "    # to be adapted.\n",
        "    meta_opt = optim.Adam(net.parameters(), lr=1e-3)\n",
        "    print(\"run MAML\")\n",
        "    log = []\n",
        "    for epoch in range(10):\n",
        "        train_maml(db, net, device, meta_opt, epoch, log)\n",
        "        test_maml(db, net, device, epoch, log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO9BksR8VoaD"
      },
      "source": [
        "## Meta training loop with support set and query set\n",
        "- Inner optimization: $\\theta^t_i = \\theta^t_{i-1} -\\alpha \\cdot \\nabla_{\\theta^t_{i-1}}\\mathcal{L}(\\theta^t_{i-1}, \\mathcal{D}^t_s)$ for $i=1, \\ldots, k$.\n",
        "- Outer optimization: \n",
        "$\\theta^t_\\text{init}= \\theta^t_{init} - \\beta \\cdot \\nabla_{\\theta_k} \\mathcal{L}(\\theta^t_k;\\mathcal{D}^t_s)\\prod_{i=1}^kI-\\alpha\\cdot \\nabla_{\\theta^t_{i-1}}(\\nabla_{\\theta^t_{i-1}}\\mathcal{L}(\\theta^t_{i-1},\\mathcal{D}^t_s))$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5PqnmXXVZk2"
      },
      "outputs": [],
      "source": [
        "# meta-train\n",
        "def train_maml(db, net, device, meta_opt, epoch, log):\n",
        "    net.train()\n",
        "    n_train_iter = db.x_train.shape[0] // db.batchsz\n",
        "\n",
        "    for batch_idx in range(n_train_iter):\n",
        "        start_time = time.time()\n",
        "        # Sample a batch of support and query images and labels.\n",
        "        batch = db.next()\n",
        "        x_spt, y_spt, x_qry, y_qry = batch\n",
        "\n",
        "        task_num, setsz, c_, h, w = x_spt.size()\n",
        "        querysz = x_qry.size(1)\n",
        "\n",
        "        # Initialize the inner optimizer to adapt the parameters to\n",
        "        # the support set.\n",
        "        n_inner_iter = 5\n",
        "        inner_opt = torch.optim.SGD(net.parameters(), lr=1e-1)\n",
        "\n",
        "        qry_losses = []\n",
        "        qry_accs = []\n",
        "        meta_opt.zero_grad()\n",
        "        for i in range(task_num):\n",
        "            with higher.innerloop_ctx(\n",
        "                net, inner_opt, copy_initial_weights=False\n",
        "            ) as (fnet, diffopt):\n",
        "                # Optimize the likelihood of the support set by taking\n",
        "                # gradient steps w.r.t. the model's parameters.\n",
        "                # This adapts the model's meta-parameters to the task.\n",
        "                # higher is able to automatically keep copies of\n",
        "                # your network's parameters as they are being updated.\n",
        "                for _ in range(n_inner_iter):\n",
        "                    spt_logits = fnet(x_spt[i])\n",
        "                    spt_loss = F.cross_entropy(spt_logits, y_spt[i])\n",
        "                    diffopt.step(spt_loss)\n",
        "\n",
        "                # The final set of adapted parameters will induce some\n",
        "                # final loss and accuracy on the query dataset.\n",
        "                # These will be used to update the model's meta-parameters.\n",
        "                qry_logits = fnet(x_qry[i])\n",
        "                qry_loss = F.cross_entropy(qry_logits, y_qry[i])\n",
        "                qry_losses.append(qry_loss.detach())\n",
        "                qry_acc = (qry_logits.argmax(\n",
        "                    dim=1) == y_qry[i]).sum().item() / querysz\n",
        "                qry_accs.append(qry_acc)\n",
        "\n",
        "                # Update the model's meta-parameters to optimize the query\n",
        "                # losses across all of the tasks sampled in this batch.\n",
        "                # This unrolls through the gradient steps.\n",
        "                qry_loss /= task_num\n",
        "                qry_loss.backward()\n",
        "\n",
        "        meta_opt.step()\n",
        "        qry_losses = sum(qry_losses) / task_num\n",
        "        qry_accs = 100. * sum(qry_accs) / task_num\n",
        "        i = epoch + float(batch_idx) / n_train_iter\n",
        "        iter_time = time.time() - start_time\n",
        "        if batch_idx % 4 == 0:\n",
        "            print(\n",
        "                f'[Epoch {i:.2f}] Train Loss: {qry_losses:.2f} | Acc: {qry_accs:.2f} | Time: {iter_time:.2f}'\n",
        "            )\n",
        "\n",
        "        log.append({\n",
        "            'epoch': i,\n",
        "            'loss': qry_losses,\n",
        "            'acc': qry_accs,\n",
        "            'mode': 'train',\n",
        "            'time': time.time(),\n",
        "        })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nM3xScVXn7A"
      },
      "source": [
        "## Meta-Test "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEGQ2xBZVeVl"
      },
      "outputs": [],
      "source": [
        "def test_maml(db, net, device, epoch, log):\n",
        "    # Crucially in our testing procedure here, we do *not* fine-tune\n",
        "    # the model during testing for simplicity.\n",
        "    # Most research papers using MAML for this task do an extra\n",
        "    # stage of fine-tuning here that should be added if you are\n",
        "    # adapting this code for research.\n",
        "    net.train()\n",
        "    n_test_iter = db.x_test.shape[0] // db.batchsz\n",
        "\n",
        "    qry_losses = []\n",
        "    qry_accs = []\n",
        "\n",
        "    for batch_idx in range(n_test_iter):\n",
        "        x_spt, y_spt, x_qry, y_qry = db.next('test')\n",
        "\n",
        "\n",
        "        task_num, setsz, c_, h, w = x_spt.size()\n",
        "        querysz = x_qry.size(1)\n",
        "\n",
        "        # TODO: Maybe pull this out into a separate module so it\n",
        "        # doesn't have to be duplicated between `train` and `test`?\n",
        "        n_inner_iter = 5\n",
        "        inner_opt = torch.optim.SGD(net.parameters(), lr=1e-1)\n",
        "\n",
        "        for i in range(task_num):\n",
        "            with higher.innerloop_ctx(net, inner_opt, track_higher_grads=False) as (fnet, diffopt):\n",
        "                # Optimize the likelihood of the support set by taking\n",
        "                # gradient steps w.r.t. the model's parameters.\n",
        "                # This adapts the model's meta-parameters to the task.\n",
        "                for _ in range(n_inner_iter):\n",
        "                    spt_logits = fnet(x_spt[i])\n",
        "                    spt_loss = F.cross_entropy(spt_logits, y_spt[i])\n",
        "                    diffopt.step(spt_loss)\n",
        "\n",
        "                # The query loss and acc induced by these parameters.\n",
        "                qry_logits = fnet(x_qry[i]).detach()\n",
        "                qry_loss = F.cross_entropy(\n",
        "                    qry_logits, y_qry[i], reduction='none')\n",
        "                qry_losses.append(qry_loss.detach())\n",
        "                qry_accs.append(\n",
        "                    (qry_logits.argmax(dim=1) == y_qry[i]).detach())\n",
        "\n",
        "    qry_losses = torch.cat(qry_losses).mean().item()\n",
        "    qry_accs = 100. * torch.cat(qry_accs).float().mean().item()\n",
        "    print(\n",
        "        f'[Epoch {epoch+1:.2f}] Test Loss: {qry_losses:.2f} | Acc: {qry_accs:.2f}'\n",
        "    )\n",
        "    log.append({\n",
        "        'epoch': epoch + 1,\n",
        "        'loss': qry_losses,\n",
        "        'acc': qry_accs,\n",
        "        'mode': 'test',\n",
        "        'time': time.time(),\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvYi8S4gUjxg"
      },
      "source": [
        "## ProtoTypical Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4J0lj3HIxKO"
      },
      "outputs": [],
      "source": [
        "class ProtoNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Sequential(\n",
        "        nn.Conv2d(1, 64, 3),\n",
        "        nn.BatchNorm2d(64, affine=True),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(2, 2),\n",
        "        nn.Conv2d(64, 64, 3),\n",
        "        nn.BatchNorm2d(64, affine=True),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(2, 2),\n",
        "        nn.Conv2d(64, 64, 3),\n",
        "        nn.BatchNorm2d(64, affine=True),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(2, 2),\n",
        "        nn.Flatten(start_dim=1)\n",
        "        )\n",
        "    \n",
        "    \n",
        "    def get_num_samples(self, labels, num_classes, dtype):\n",
        "        b = labels.size(0)\n",
        "        with torch.no_grad():\n",
        "            ones = torch.ones_like(labels, dtype=dtype)\n",
        "            num_samples = ones.new_zeros((b, num_classes))\n",
        "            num_samples.scatter_add_(1, labels, ones)\n",
        "        return num_samples\n",
        "\n",
        "    def make_prototypes(self, embeddings, labels, nways):\n",
        "        B, _, H = embeddings.size()\n",
        "\n",
        "        num_samples = self.get_num_samples(\n",
        "            labels=labels, num_classes=nways, dtype=embeddings.dtype)\n",
        "\n",
        "        num_samples.unsqueeze_(-1)\n",
        "        num_samples = torch.max(num_samples, torch.ones_like(num_samples))\n",
        "\n",
        "        prototypes = embeddings.new_zeros((B, nways, H))\n",
        "        indices = labels.unsqueeze(-1).expand_as(embeddings)\n",
        "        prototypes.scatter_add_(1, indices, embeddings).div_(num_samples)\n",
        "        \n",
        "        return prototypes\n",
        "\n",
        "    def prototypical_loss(self, prototypes, embeddings, labels):\n",
        "        # prototypes: [b, n_way, d]\n",
        "        # embeddigns: [b, n_way*q_shots, d]\n",
        "        # labels: [b, nway*q_shots]\n",
        "        # distances: [b, nway*q_shots, nway*q_shots]\n",
        "        \n",
        "        sqr_dist = (prototypes.unsqueeze(2) - embeddings.unsqueeze(1)) ** 2\n",
        "        distances = torch.sum(sqr_dist, dim=-1)\n",
        "        return F.cross_entropy(-distances, labels)\n",
        "        \n",
        "    def get_accuracy(self, prototypes, embeddings, labels):\n",
        "        sq_distances = torch.sum((prototypes.unsqueeze(\n",
        "            1) - embeddings.unsqueeze(2)) ** 2, dim=-1)\n",
        "        _, preds = torch.min(sq_distances, dim=-1)\n",
        "        return torch.mean(preds.eq(labels).float(), 1).mean(0)\n",
        "    \n",
        "    def forward(self, x_spt, y_spt, x_qry, y_qry):\n",
        "        b, s_size, c, h, w = x_spt.size()\n",
        "        _, q_size, _, _, _ = x_qry.size()\n",
        "        \n",
        "        s_embed = self.embedding(x_spt.view(b*s_size, c, h, w))\n",
        "        s_embed = s_embed.view(b, s_size, -1)\n",
        "\n",
        "        q_embed = self.embedding(x_qry.view(b*q_size, c, h, w))\n",
        "        q_embed = q_embed.view(b, q_size, -1)\n",
        "        \n",
        "        # Create the prototypes\n",
        "        prototypes = self.make_prototypes(embeddings=s_embed, \n",
        "                                          labels=y_spt.view(b, s_size), \n",
        "                                          nways=5)\n",
        "\n",
        "        loss = self.prototypical_loss(prototypes=prototypes, \n",
        "                                      embeddings=q_embed, \n",
        "                                      labels=y_qry.view(b, q_size))\n",
        "\n",
        "        acc = self.get_accuracy(prototypes=prototypes, \n",
        "                                embeddings=q_embed, \n",
        "                                labels=y_qry.view(b, q_size))\n",
        "        return loss, acc               "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiyYE21gLN0P"
      },
      "outputs": [],
      "source": [
        "def run_protonet():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--n_way', type=int, help='n way', default=5)\n",
        "    parser.add_argument(\n",
        "        '--k_spt', type=int, help='k shot for support set', default=5)\n",
        "    parser.add_argument(\n",
        "        '--k_qry', type=int, help='k shot for query set', default=15)\n",
        "    parser.add_argument(\n",
        "        '--task_num',\n",
        "        type=int,\n",
        "        help='meta batch size, namely task num',\n",
        "        default=32)\n",
        "    parser.add_argument('--seed', type=int, help='random seed', default=1004)\n",
        "    args = parser.parse_args(args=[])\n",
        "\n",
        "    torch.manual_seed(args.seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "\n",
        "    # Set up the Omniglot loader.\n",
        "    device = torch.cuda.current_device()\n",
        "    db = OmniglotNShot(\n",
        "        './tmp/omniglot-data',\n",
        "        batchsz=args.task_num,\n",
        "        n_way=args.n_way,\n",
        "        k_shot=args.k_spt,\n",
        "        k_query=args.k_qry,\n",
        "        imgsz=28,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    # Create a vanilla PyTorch neural network that will be\n",
        "    # automatically monkey-patched by higher later.\n",
        "    # Before higher, models could *not* be created like this\n",
        "    # and the parameters needed to be manually updated and copied\n",
        "    # for the updates.\n",
        "    net = ProtoNet().to(device)\n",
        "    # We will use Adam to (meta-)optimize the initial parameters\n",
        "    # to be adapted.\n",
        "    opt = optim.Adam(net.parameters(), lr=1e-3)\n",
        "    print(\"run ProtoTypical Network\")\n",
        "    log = []\n",
        "    for epoch in range(20):\n",
        "        train_protonet(db, net, device, opt, epoch, log)\n",
        "        test_protonet(db, net, device, epoch, log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTqu7TKeYPHl"
      },
      "outputs": [],
      "source": [
        "# meta-train\n",
        "def train_protonet(db, net, device, opt, epoch, log):\n",
        "    net.train()\n",
        "    n_train_iter = db.x_train.shape[0] // db.batchsz\n",
        "\n",
        "    for batch_idx in range(n_train_iter):\n",
        "        start_time = time.time()\n",
        "        # Sample a batch of support and query images and labels.\n",
        "        batch = db.next()\n",
        "        x_spt, y_spt, x_qry, y_qry = batch\n",
        "        task_num, setsz, c_, h, w = x_spt.size()\n",
        "        querysz = x_qry.size(1)\n",
        "\n",
        "        loss, acc = net(x_spt, y_spt, x_qry, y_qry)\n",
        "\n",
        "        qry_losses = []\n",
        "        qry_accs = []\n",
        "        \n",
        "        net.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        \n",
        "        qry_accs = 100. * acc\n",
        "        i = epoch + float(batch_idx) / n_train_iter\n",
        "        iter_time = time.time() - start_time\n",
        "        if batch_idx % 4 == 0:\n",
        "            print(\n",
        "                f'[Epoch {i:.2f}] Train Loss: {loss.item():.2f} | Acc: {qry_accs:.2f} | Time: {iter_time:.2f}'\n",
        "            )\n",
        "\n",
        "        log.append({\n",
        "            'epoch': i,\n",
        "            'loss': loss.item(),\n",
        "            'acc': qry_accs,\n",
        "            'mode': 'train',\n",
        "            'time': time.time(),\n",
        "        })\n",
        "\n",
        "\n",
        "def test_protonet(db, net, device, epoch, log):\n",
        "    net.eval()\n",
        "    n_test_iter = db.x_test.shape[0] // db.batchsz\n",
        "\n",
        "    qry_losses = []\n",
        "    qry_accs = []\n",
        "\n",
        "    for batch_idx in range(n_test_iter):\n",
        "        x_spt, y_spt, x_qry, y_qry = db.next('test')\n",
        "        task_num, setsz, c_, h, w = x_spt.size()\n",
        "        querysz = x_qry.size(1)\n",
        "        with torch.no_grad():\n",
        "            loss, acc = net(x_spt, y_spt, x_qry, y_qry)\n",
        "\n",
        "            qry_losses.append(loss.item())\n",
        "            qry_accs.append(acc.item())\n",
        "\n",
        "    qry_losses = np.mean(qry_losses)\n",
        "    qry_accs = 100. * np.mean(qry_accs)\n",
        "    print(\n",
        "        f'[Epoch {epoch+1:.2f}] Test Loss: {qry_losses:.2f} | Acc: {qry_accs:.2f}'\n",
        "    )\n",
        "    log.append({\n",
        "        'epoch': epoch + 1,\n",
        "        'loss': qry_losses,\n",
        "        'acc': qry_accs,\n",
        "        'mode': 'test',\n",
        "        'time': time.time(),\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j14vlAgf7Zvs",
        "outputId": "1300a17a-12ab-4ea1-dea5-5d8bc2cb5d06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load from omniglot.npy.\n",
            "DB: train (1200, 20, 1, 28, 28) test (423, 20, 1, 28, 28)\n",
            "run ProtoTypical Network\n",
            "[Epoch 0.00] Train Loss: 1.19 | Acc: 80.38 | Time: 1.76\n",
            "[Epoch 0.11] Train Loss: 0.58 | Acc: 88.54 | Time: 0.01\n",
            "[Epoch 0.22] Train Loss: 0.40 | Acc: 92.63 | Time: 0.01\n",
            "[Epoch 0.32] Train Loss: 0.22 | Acc: 95.25 | Time: 0.02\n",
            "[Epoch 0.43] Train Loss: 0.31 | Acc: 93.79 | Time: 0.01\n",
            "[Epoch 0.54] Train Loss: 0.19 | Acc: 95.92 | Time: 0.48\n",
            "[Epoch 0.65] Train Loss: 0.23 | Acc: 95.00 | Time: 0.02\n",
            "[Epoch 0.76] Train Loss: 0.25 | Acc: 95.25 | Time: 0.02\n",
            "[Epoch 0.86] Train Loss: 0.17 | Acc: 95.79 | Time: 0.01\n",
            "[Epoch 0.97] Train Loss: 0.14 | Acc: 96.83 | Time: 0.01\n",
            "[Epoch 1.00] Test Loss: 0.33 | Acc: 93.21\n",
            "[Epoch 1.00] Train Loss: 0.15 | Acc: 96.17 | Time: 0.01\n",
            "[Epoch 1.11] Train Loss: 0.18 | Acc: 96.13 | Time: 0.01\n",
            "[Epoch 1.22] Train Loss: 0.14 | Acc: 96.67 | Time: 0.03\n",
            "[Epoch 1.32] Train Loss: 0.13 | Acc: 97.08 | Time: 0.03\n",
            "[Epoch 1.43] Train Loss: 0.08 | Acc: 98.17 | Time: 0.01\n",
            "[Epoch 1.54] Train Loss: 0.10 | Acc: 97.46 | Time: 0.01\n",
            "[Epoch 1.65] Train Loss: 0.13 | Acc: 97.04 | Time: 0.01\n",
            "[Epoch 1.76] Train Loss: 0.11 | Acc: 97.17 | Time: 0.01\n",
            "[Epoch 1.86] Train Loss: 0.09 | Acc: 98.13 | Time: 0.01\n",
            "[Epoch 1.97] Train Loss: 0.12 | Acc: 97.00 | Time: 0.01\n",
            "[Epoch 2.00] Test Loss: 0.24 | Acc: 94.87\n",
            "[Epoch 2.00] Train Loss: 0.09 | Acc: 97.63 | Time: 0.01\n",
            "[Epoch 2.11] Train Loss: 0.12 | Acc: 97.04 | Time: 0.01\n",
            "[Epoch 2.22] Train Loss: 0.06 | Acc: 98.29 | Time: 0.01\n",
            "[Epoch 2.32] Train Loss: 0.08 | Acc: 98.04 | Time: 0.01\n",
            "[Epoch 2.43] Train Loss: 0.11 | Acc: 97.13 | Time: 0.31\n",
            "[Epoch 2.54] Train Loss: 0.10 | Acc: 97.54 | Time: 0.01\n",
            "[Epoch 2.65] Train Loss: 0.09 | Acc: 97.96 | Time: 0.01\n",
            "[Epoch 2.76] Train Loss: 0.10 | Acc: 97.46 | Time: 0.01\n",
            "[Epoch 2.86] Train Loss: 0.08 | Acc: 97.75 | Time: 0.01\n",
            "[Epoch 2.97] Train Loss: 0.12 | Acc: 97.13 | Time: 0.31\n",
            "[Epoch 3.00] Test Loss: 0.21 | Acc: 95.28\n",
            "[Epoch 3.00] Train Loss: 0.06 | Acc: 98.63 | Time: 0.01\n",
            "[Epoch 3.11] Train Loss: 0.09 | Acc: 97.25 | Time: 0.01\n",
            "[Epoch 3.22] Train Loss: 0.11 | Acc: 97.50 | Time: 0.01\n",
            "[Epoch 3.32] Train Loss: 0.05 | Acc: 98.38 | Time: 0.01\n",
            "[Epoch 3.43] Train Loss: 0.07 | Acc: 97.83 | Time: 0.01\n",
            "[Epoch 3.54] Train Loss: 0.07 | Acc: 98.17 | Time: 0.01\n",
            "[Epoch 3.65] Train Loss: 0.08 | Acc: 97.58 | Time: 0.01\n",
            "[Epoch 3.76] Train Loss: 0.08 | Acc: 97.88 | Time: 0.01\n",
            "[Epoch 3.86] Train Loss: 0.06 | Acc: 98.13 | Time: 0.01\n",
            "[Epoch 3.97] Train Loss: 0.06 | Acc: 98.21 | Time: 0.01\n",
            "[Epoch 4.00] Test Loss: 0.18 | Acc: 95.60\n",
            "[Epoch 4.00] Train Loss: 0.08 | Acc: 97.79 | Time: 0.01\n",
            "[Epoch 4.11] Train Loss: 0.05 | Acc: 98.50 | Time: 0.01\n",
            "[Epoch 4.22] Train Loss: 0.09 | Acc: 97.75 | Time: 0.01\n",
            "[Epoch 4.32] Train Loss: 0.06 | Acc: 98.29 | Time: 0.30\n",
            "[Epoch 4.43] Train Loss: 0.06 | Acc: 98.75 | Time: 0.01\n",
            "[Epoch 4.54] Train Loss: 0.08 | Acc: 97.67 | Time: 0.01\n",
            "[Epoch 4.65] Train Loss: 0.06 | Acc: 98.33 | Time: 0.01\n",
            "[Epoch 4.76] Train Loss: 0.06 | Acc: 97.83 | Time: 0.01\n",
            "[Epoch 4.86] Train Loss: 0.06 | Acc: 98.38 | Time: 0.31\n",
            "[Epoch 4.97] Train Loss: 0.04 | Acc: 98.67 | Time: 0.01\n",
            "[Epoch 5.00] Test Loss: 0.19 | Acc: 95.48\n",
            "[Epoch 5.00] Train Loss: 0.05 | Acc: 98.75 | Time: 0.01\n",
            "[Epoch 5.11] Train Loss: 0.06 | Acc: 98.29 | Time: 0.01\n",
            "[Epoch 5.22] Train Loss: 0.05 | Acc: 98.50 | Time: 0.01\n",
            "[Epoch 5.32] Train Loss: 0.04 | Acc: 98.67 | Time: 0.01\n",
            "[Epoch 5.43] Train Loss: 0.04 | Acc: 98.54 | Time: 0.01\n",
            "[Epoch 5.54] Train Loss: 0.07 | Acc: 97.88 | Time: 0.01\n",
            "[Epoch 5.65] Train Loss: 0.05 | Acc: 98.46 | Time: 0.01\n",
            "[Epoch 5.76] Train Loss: 0.07 | Acc: 97.96 | Time: 0.01\n",
            "[Epoch 5.86] Train Loss: 0.05 | Acc: 98.79 | Time: 0.01\n",
            "[Epoch 5.97] Train Loss: 0.05 | Acc: 98.63 | Time: 0.01\n",
            "[Epoch 6.00] Test Loss: 0.17 | Acc: 95.99\n",
            "[Epoch 6.00] Train Loss: 0.04 | Acc: 98.88 | Time: 0.01\n",
            "[Epoch 6.11] Train Loss: 0.04 | Acc: 98.79 | Time: 0.01\n",
            "[Epoch 6.22] Train Loss: 0.04 | Acc: 98.50 | Time: 0.31\n",
            "[Epoch 6.32] Train Loss: 0.04 | Acc: 98.71 | Time: 0.01\n",
            "[Epoch 6.43] Train Loss: 0.03 | Acc: 98.92 | Time: 0.01\n",
            "[Epoch 6.54] Train Loss: 0.05 | Acc: 98.38 | Time: 0.01\n",
            "[Epoch 6.65] Train Loss: 0.04 | Acc: 98.83 | Time: 0.01\n",
            "[Epoch 6.76] Train Loss: 0.06 | Acc: 98.33 | Time: 0.30\n",
            "[Epoch 6.86] Train Loss: 0.06 | Acc: 98.21 | Time: 0.01\n",
            "[Epoch 6.97] Train Loss: 0.04 | Acc: 99.00 | Time: 0.01\n",
            "[Epoch 7.00] Test Loss: 0.15 | Acc: 95.99\n",
            "[Epoch 7.00] Train Loss: 0.07 | Acc: 97.88 | Time: 0.01\n",
            "[Epoch 7.11] Train Loss: 0.05 | Acc: 98.58 | Time: 0.01\n",
            "[Epoch 7.22] Train Loss: 0.03 | Acc: 99.17 | Time: 0.01\n",
            "[Epoch 7.32] Train Loss: 0.05 | Acc: 98.13 | Time: 0.01\n",
            "[Epoch 7.43] Train Loss: 0.04 | Acc: 98.88 | Time: 0.01\n",
            "[Epoch 7.54] Train Loss: 0.04 | Acc: 98.79 | Time: 0.01\n",
            "[Epoch 7.65] Train Loss: 0.07 | Acc: 97.42 | Time: 0.01\n",
            "[Epoch 7.76] Train Loss: 0.05 | Acc: 98.25 | Time: 0.01\n",
            "[Epoch 7.86] Train Loss: 0.02 | Acc: 99.46 | Time: 0.01\n",
            "[Epoch 7.97] Train Loss: 0.04 | Acc: 98.79 | Time: 0.01\n",
            "[Epoch 8.00] Test Loss: 0.14 | Acc: 96.14\n",
            "[Epoch 8.00] Train Loss: 0.04 | Acc: 98.79 | Time: 0.01\n",
            "[Epoch 8.11] Train Loss: 0.05 | Acc: 98.63 | Time: 0.31\n",
            "[Epoch 8.22] Train Loss: 0.04 | Acc: 98.63 | Time: 0.01\n",
            "[Epoch 8.32] Train Loss: 0.04 | Acc: 98.50 | Time: 0.01\n",
            "[Epoch 8.43] Train Loss: 0.03 | Acc: 98.96 | Time: 0.01\n",
            "[Epoch 8.54] Train Loss: 0.02 | Acc: 99.17 | Time: 0.01\n",
            "[Epoch 8.65] Train Loss: 0.05 | Acc: 98.67 | Time: 0.32\n",
            "[Epoch 8.76] Train Loss: 0.02 | Acc: 99.29 | Time: 0.01\n",
            "[Epoch 8.86] Train Loss: 0.05 | Acc: 98.71 | Time: 0.01\n",
            "[Epoch 8.97] Train Loss: 0.03 | Acc: 99.08 | Time: 0.01\n",
            "[Epoch 9.00] Test Loss: 0.15 | Acc: 96.31\n",
            "[Epoch 9.00] Train Loss: 0.05 | Acc: 98.33 | Time: 0.01\n",
            "[Epoch 9.11] Train Loss: 0.03 | Acc: 99.08 | Time: 0.01\n",
            "[Epoch 9.22] Train Loss: 0.04 | Acc: 98.54 | Time: 0.01\n",
            "[Epoch 9.32] Train Loss: 0.03 | Acc: 98.92 | Time: 0.01\n",
            "[Epoch 9.43] Train Loss: 0.02 | Acc: 99.21 | Time: 0.01\n",
            "[Epoch 9.54] Train Loss: 0.05 | Acc: 98.67 | Time: 0.01\n",
            "[Epoch 9.65] Train Loss: 0.03 | Acc: 99.04 | Time: 0.01\n",
            "[Epoch 9.76] Train Loss: 0.05 | Acc: 98.50 | Time: 0.01\n",
            "[Epoch 9.86] Train Loss: 0.04 | Acc: 98.83 | Time: 0.01\n",
            "[Epoch 9.97] Train Loss: 0.04 | Acc: 98.54 | Time: 0.01\n",
            "[Epoch 10.00] Test Loss: 0.13 | Acc: 96.81\n",
            "[Epoch 10.00] Train Loss: 0.04 | Acc: 98.46 | Time: 0.31\n",
            "[Epoch 10.11] Train Loss: 0.04 | Acc: 98.92 | Time: 0.01\n",
            "[Epoch 10.22] Train Loss: 0.03 | Acc: 98.96 | Time: 0.01\n",
            "[Epoch 10.32] Train Loss: 0.03 | Acc: 99.04 | Time: 0.01\n",
            "[Epoch 10.43] Train Loss: 0.03 | Acc: 98.88 | Time: 0.01\n",
            "[Epoch 10.54] Train Loss: 0.03 | Acc: 99.42 | Time: 0.30\n",
            "[Epoch 10.65] Train Loss: 0.03 | Acc: 99.17 | Time: 0.01\n",
            "[Epoch 10.76] Train Loss: 0.03 | Acc: 99.21 | Time: 0.01\n",
            "[Epoch 10.86] Train Loss: 0.02 | Acc: 99.25 | Time: 0.01\n",
            "[Epoch 10.97] Train Loss: 0.03 | Acc: 98.83 | Time: 0.01\n",
            "[Epoch 11.00] Test Loss: 0.14 | Acc: 96.47\n",
            "[Epoch 11.00] Train Loss: 0.02 | Acc: 99.08 | Time: 0.01\n",
            "[Epoch 11.11] Train Loss: 0.02 | Acc: 99.29 | Time: 0.01\n",
            "[Epoch 11.22] Train Loss: 0.02 | Acc: 99.42 | Time: 0.01\n",
            "[Epoch 11.32] Train Loss: 0.04 | Acc: 98.71 | Time: 0.01\n",
            "[Epoch 11.43] Train Loss: 0.03 | Acc: 99.04 | Time: 0.01\n",
            "[Epoch 11.54] Train Loss: 0.03 | Acc: 99.04 | Time: 0.01\n",
            "[Epoch 11.65] Train Loss: 0.04 | Acc: 98.92 | Time: 0.01\n",
            "[Epoch 11.76] Train Loss: 0.03 | Acc: 99.12 | Time: 0.01\n",
            "[Epoch 11.86] Train Loss: 0.02 | Acc: 99.08 | Time: 0.01\n",
            "[Epoch 11.97] Train Loss: 0.02 | Acc: 99.12 | Time: 0.01\n",
            "[Epoch 12.00] Test Loss: 0.15 | Acc: 96.49\n",
            "[Epoch 12.00] Train Loss: 0.03 | Acc: 99.00 | Time: 0.01\n",
            "[Epoch 12.11] Train Loss: 0.02 | Acc: 99.46 | Time: 0.01\n",
            "[Epoch 12.22] Train Loss: 0.03 | Acc: 99.00 | Time: 0.01\n",
            "[Epoch 12.32] Train Loss: 0.03 | Acc: 98.75 | Time: 0.01\n",
            "[Epoch 12.43] Train Loss: 0.03 | Acc: 99.17 | Time: 0.30\n",
            "[Epoch 12.54] Train Loss: 0.04 | Acc: 98.38 | Time: 0.01\n",
            "[Epoch 12.65] Train Loss: 0.02 | Acc: 99.38 | Time: 0.01\n",
            "[Epoch 12.76] Train Loss: 0.03 | Acc: 99.17 | Time: 0.01\n",
            "[Epoch 12.86] Train Loss: 0.03 | Acc: 99.12 | Time: 0.01\n",
            "[Epoch 12.97] Train Loss: 0.04 | Acc: 98.88 | Time: 0.30\n",
            "[Epoch 13.00] Test Loss: 0.12 | Acc: 96.82\n",
            "[Epoch 13.00] Train Loss: 0.03 | Acc: 98.88 | Time: 0.01\n",
            "[Epoch 13.11] Train Loss: 0.02 | Acc: 99.29 | Time: 0.02\n",
            "[Epoch 13.22] Train Loss: 0.04 | Acc: 98.63 | Time: 0.01\n",
            "[Epoch 13.32] Train Loss: 0.02 | Acc: 99.00 | Time: 0.01\n",
            "[Epoch 13.43] Train Loss: 0.02 | Acc: 99.50 | Time: 0.01\n",
            "[Epoch 13.54] Train Loss: 0.03 | Acc: 99.12 | Time: 0.01\n",
            "[Epoch 13.65] Train Loss: 0.04 | Acc: 98.79 | Time: 0.01\n",
            "[Epoch 13.76] Train Loss: 0.02 | Acc: 99.25 | Time: 0.01\n",
            "[Epoch 13.86] Train Loss: 0.03 | Acc: 98.67 | Time: 0.01\n",
            "[Epoch 13.97] Train Loss: 0.02 | Acc: 99.21 | Time: 0.01\n",
            "[Epoch 14.00] Test Loss: 0.13 | Acc: 96.85\n",
            "[Epoch 14.00] Train Loss: 0.01 | Acc: 99.62 | Time: 0.01\n",
            "[Epoch 14.11] Train Loss: 0.03 | Acc: 98.88 | Time: 0.01\n",
            "[Epoch 14.22] Train Loss: 0.02 | Acc: 99.38 | Time: 0.01\n",
            "[Epoch 14.32] Train Loss: 0.01 | Acc: 99.62 | Time: 0.30\n",
            "[Epoch 14.43] Train Loss: 0.04 | Acc: 99.12 | Time: 0.01\n",
            "[Epoch 14.54] Train Loss: 0.02 | Acc: 99.50 | Time: 0.01\n",
            "[Epoch 14.65] Train Loss: 0.03 | Acc: 99.04 | Time: 0.01\n",
            "[Epoch 14.76] Train Loss: 0.03 | Acc: 99.17 | Time: 0.01\n",
            "[Epoch 14.86] Train Loss: 0.04 | Acc: 99.12 | Time: 0.30\n",
            "[Epoch 14.97] Train Loss: 0.03 | Acc: 98.88 | Time: 0.01\n",
            "[Epoch 15.00] Test Loss: 0.12 | Acc: 97.05\n",
            "[Epoch 15.00] Train Loss: 0.03 | Acc: 98.79 | Time: 0.01\n",
            "[Epoch 15.11] Train Loss: 0.03 | Acc: 98.71 | Time: 0.01\n",
            "[Epoch 15.22] Train Loss: 0.02 | Acc: 99.33 | Time: 0.01\n",
            "[Epoch 15.32] Train Loss: 0.02 | Acc: 99.42 | Time: 0.01\n",
            "[Epoch 15.43] Train Loss: 0.03 | Acc: 99.33 | Time: 0.01\n",
            "[Epoch 15.54] Train Loss: 0.03 | Acc: 99.08 | Time: 0.01\n",
            "[Epoch 15.65] Train Loss: 0.02 | Acc: 99.38 | Time: 0.01\n",
            "[Epoch 15.76] Train Loss: 0.02 | Acc: 99.33 | Time: 0.01\n",
            "[Epoch 15.86] Train Loss: 0.02 | Acc: 99.38 | Time: 0.01\n",
            "[Epoch 15.97] Train Loss: 0.02 | Acc: 99.38 | Time: 0.01\n",
            "[Epoch 16.00] Test Loss: 0.13 | Acc: 96.79\n",
            "[Epoch 16.00] Train Loss: 0.03 | Acc: 98.75 | Time: 0.01\n",
            "[Epoch 16.11] Train Loss: 0.03 | Acc: 99.08 | Time: 0.01\n",
            "[Epoch 16.22] Train Loss: 0.01 | Acc: 99.58 | Time: 0.31\n",
            "[Epoch 16.32] Train Loss: 0.02 | Acc: 99.12 | Time: 0.01\n",
            "[Epoch 16.43] Train Loss: 0.02 | Acc: 99.33 | Time: 0.01\n",
            "[Epoch 16.54] Train Loss: 0.03 | Acc: 99.00 | Time: 0.01\n",
            "[Epoch 16.65] Train Loss: 0.01 | Acc: 99.38 | Time: 0.01\n",
            "[Epoch 16.76] Train Loss: 0.01 | Acc: 99.50 | Time: 0.31\n",
            "[Epoch 16.86] Train Loss: 0.02 | Acc: 99.29 | Time: 0.01\n",
            "[Epoch 16.97] Train Loss: 0.01 | Acc: 99.62 | Time: 0.01\n",
            "[Epoch 17.00] Test Loss: 0.12 | Acc: 97.11\n",
            "[Epoch 17.00] Train Loss: 0.03 | Acc: 99.04 | Time: 0.01\n",
            "[Epoch 17.11] Train Loss: 0.02 | Acc: 99.42 | Time: 0.01\n",
            "[Epoch 17.22] Train Loss: 0.04 | Acc: 98.96 | Time: 0.01\n",
            "[Epoch 17.32] Train Loss: 0.02 | Acc: 99.29 | Time: 0.01\n",
            "[Epoch 17.43] Train Loss: 0.02 | Acc: 99.29 | Time: 0.01\n",
            "[Epoch 17.54] Train Loss: 0.01 | Acc: 99.46 | Time: 0.01\n",
            "[Epoch 17.65] Train Loss: 0.02 | Acc: 99.17 | Time: 0.01\n",
            "[Epoch 17.76] Train Loss: 0.02 | Acc: 99.25 | Time: 0.01\n",
            "[Epoch 17.86] Train Loss: 0.02 | Acc: 99.12 | Time: 0.01\n",
            "[Epoch 17.97] Train Loss: 0.02 | Acc: 99.04 | Time: 0.01\n",
            "[Epoch 18.00] Test Loss: 0.11 | Acc: 97.07\n",
            "[Epoch 18.00] Train Loss: 0.01 | Acc: 99.79 | Time: 0.01\n",
            "[Epoch 18.11] Train Loss: 0.02 | Acc: 99.33 | Time: 0.30\n",
            "[Epoch 18.22] Train Loss: 0.02 | Acc: 99.54 | Time: 0.01\n",
            "[Epoch 18.32] Train Loss: 0.03 | Acc: 99.12 | Time: 0.01\n",
            "[Epoch 18.43] Train Loss: 0.02 | Acc: 99.50 | Time: 0.01\n",
            "[Epoch 18.54] Train Loss: 0.02 | Acc: 99.25 | Time: 0.01\n",
            "[Epoch 18.65] Train Loss: 0.03 | Acc: 99.12 | Time: 0.31\n",
            "[Epoch 18.76] Train Loss: 0.01 | Acc: 99.71 | Time: 0.01\n",
            "[Epoch 18.86] Train Loss: 0.01 | Acc: 99.46 | Time: 0.01\n",
            "[Epoch 18.97] Train Loss: 0.04 | Acc: 99.17 | Time: 0.01\n",
            "[Epoch 19.00] Test Loss: 0.13 | Acc: 96.88\n",
            "[Epoch 19.00] Train Loss: 0.02 | Acc: 99.12 | Time: 0.01\n",
            "[Epoch 19.11] Train Loss: 0.03 | Acc: 99.21 | Time: 0.01\n",
            "[Epoch 19.22] Train Loss: 0.02 | Acc: 99.29 | Time: 0.01\n",
            "[Epoch 19.32] Train Loss: 0.02 | Acc: 99.50 | Time: 0.01\n",
            "[Epoch 19.43] Train Loss: 0.02 | Acc: 99.38 | Time: 0.01\n",
            "[Epoch 19.54] Train Loss: 0.02 | Acc: 99.46 | Time: 0.01\n",
            "[Epoch 19.65] Train Loss: 0.03 | Acc: 99.04 | Time: 0.01\n",
            "[Epoch 19.76] Train Loss: 0.02 | Acc: 99.42 | Time: 0.01\n",
            "[Epoch 19.86] Train Loss: 0.02 | Acc: 99.38 | Time: 0.01\n",
            "[Epoch 19.97] Train Loss: 0.02 | Acc: 99.00 | Time: 0.01\n",
            "[Epoch 20.00] Test Loss: 0.12 | Acc: 96.86\n"
          ]
        }
      ],
      "source": [
        "run_protonet()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dEzQrJKUPDw",
        "outputId": "6a9d1fea-ff48-4fa9-b260-04ba5ccf443c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load from omniglot.npy.\n",
            "DB: train (1200, 20, 1, 28, 28) test (423, 20, 1, 28, 28)\n",
            "run MAML\n",
            "[Epoch 0.00] Train Loss: 0.01 | Acc: 89.92 | Time: 1.65\n",
            "[Epoch 0.11] Train Loss: 0.01 | Acc: 93.96 | Time: 2.37\n",
            "[Epoch 0.22] Train Loss: 0.01 | Acc: 93.75 | Time: 1.43\n",
            "[Epoch 0.32] Train Loss: 0.01 | Acc: 96.50 | Time: 1.43\n",
            "[Epoch 0.43] Train Loss: 0.01 | Acc: 96.88 | Time: 1.45\n",
            "[Epoch 0.54] Train Loss: 0.01 | Acc: 97.04 | Time: 1.74\n",
            "[Epoch 0.65] Train Loss: 0.01 | Acc: 96.58 | Time: 1.45\n",
            "[Epoch 0.76] Train Loss: 0.01 | Acc: 96.46 | Time: 1.46\n",
            "[Epoch 0.86] Train Loss: 0.01 | Acc: 96.62 | Time: 1.88\n",
            "[Epoch 0.97] Train Loss: 0.01 | Acc: 97.58 | Time: 1.41\n",
            "[Epoch 1.00] Test Loss: 0.23 | Acc: 95.20\n",
            "[Epoch 1.00] Train Loss: 0.01 | Acc: 96.87 | Time: 1.43\n",
            "[Epoch 1.11] Train Loss: 0.01 | Acc: 97.29 | Time: 1.43\n",
            "[Epoch 1.22] Train Loss: 0.00 | Acc: 96.71 | Time: 1.78\n",
            "[Epoch 1.32] Train Loss: 0.00 | Acc: 97.25 | Time: 1.44\n",
            "[Epoch 1.43] Train Loss: 0.00 | Acc: 97.71 | Time: 1.43\n",
            "[Epoch 1.54] Train Loss: 0.00 | Acc: 97.25 | Time: 1.41\n",
            "[Epoch 1.65] Train Loss: 0.00 | Acc: 96.88 | Time: 1.45\n",
            "[Epoch 1.76] Train Loss: 0.00 | Acc: 97.50 | Time: 1.41\n",
            "[Epoch 1.86] Train Loss: 0.00 | Acc: 97.92 | Time: 1.42\n",
            "[Epoch 1.97] Train Loss: 0.00 | Acc: 97.04 | Time: 1.43\n",
            "[Epoch 2.00] Test Loss: 0.17 | Acc: 95.85\n",
            "[Epoch 2.00] Train Loss: 0.00 | Acc: 97.38 | Time: 1.43\n",
            "[Epoch 2.11] Train Loss: 0.00 | Acc: 97.38 | Time: 1.52\n",
            "[Epoch 2.22] Train Loss: 0.00 | Acc: 98.04 | Time: 1.41\n",
            "[Epoch 2.32] Train Loss: 0.00 | Acc: 97.92 | Time: 1.42\n",
            "[Epoch 2.43] Train Loss: 0.00 | Acc: 97.12 | Time: 1.74\n",
            "[Epoch 2.54] Train Loss: 0.00 | Acc: 97.79 | Time: 1.45\n",
            "[Epoch 2.65] Train Loss: 0.00 | Acc: 97.38 | Time: 1.50\n",
            "[Epoch 2.76] Train Loss: 0.00 | Acc: 98.00 | Time: 1.53\n",
            "[Epoch 2.86] Train Loss: 0.00 | Acc: 97.54 | Time: 1.42\n",
            "[Epoch 2.97] Train Loss: 0.00 | Acc: 97.88 | Time: 1.76\n",
            "[Epoch 3.00] Test Loss: 0.15 | Acc: 96.42\n",
            "[Epoch 3.00] Train Loss: 0.00 | Acc: 97.83 | Time: 1.40\n",
            "[Epoch 3.11] Train Loss: 0.00 | Acc: 97.29 | Time: 1.42\n",
            "[Epoch 3.22] Train Loss: 0.00 | Acc: 97.58 | Time: 1.45\n",
            "[Epoch 3.32] Train Loss: 0.00 | Acc: 97.71 | Time: 1.43\n",
            "[Epoch 3.43] Train Loss: 0.00 | Acc: 97.21 | Time: 1.42\n",
            "[Epoch 3.54] Train Loss: 0.00 | Acc: 97.42 | Time: 1.43\n",
            "[Epoch 3.65] Train Loss: 0.00 | Acc: 97.25 | Time: 1.41\n",
            "[Epoch 3.76] Train Loss: 0.00 | Acc: 97.46 | Time: 1.45\n",
            "[Epoch 3.86] Train Loss: 0.00 | Acc: 98.13 | Time: 1.42\n",
            "[Epoch 3.97] Train Loss: 0.00 | Acc: 97.71 | Time: 1.45\n",
            "[Epoch 4.00] Test Loss: 0.13 | Acc: 96.65\n",
            "[Epoch 4.00] Train Loss: 0.00 | Acc: 97.87 | Time: 1.43\n",
            "[Epoch 4.11] Train Loss: 0.00 | Acc: 98.42 | Time: 1.43\n",
            "[Epoch 4.22] Train Loss: 0.00 | Acc: 97.13 | Time: 1.43\n",
            "[Epoch 4.32] Train Loss: 0.00 | Acc: 97.17 | Time: 1.73\n",
            "[Epoch 4.43] Train Loss: 0.00 | Acc: 96.21 | Time: 1.44\n",
            "[Epoch 4.54] Train Loss: 0.01 | Acc: 95.54 | Time: 1.41\n",
            "[Epoch 4.65] Train Loss: 0.00 | Acc: 96.62 | Time: 1.41\n",
            "[Epoch 4.76] Train Loss: 0.00 | Acc: 96.00 | Time: 1.45\n",
            "[Epoch 4.86] Train Loss: 0.00 | Acc: 97.38 | Time: 1.75\n",
            "[Epoch 4.97] Train Loss: 0.00 | Acc: 97.46 | Time: 1.44\n",
            "[Epoch 5.00] Test Loss: 0.17 | Acc: 95.55\n",
            "[Epoch 5.00] Train Loss: 0.00 | Acc: 97.38 | Time: 1.51\n",
            "[Epoch 5.11] Train Loss: 0.00 | Acc: 96.58 | Time: 1.43\n",
            "[Epoch 5.22] Train Loss: 0.00 | Acc: 97.00 | Time: 1.43\n",
            "[Epoch 5.32] Train Loss: 0.00 | Acc: 97.67 | Time: 1.44\n",
            "[Epoch 5.43] Train Loss: 0.00 | Acc: 98.33 | Time: 1.43\n",
            "[Epoch 5.54] Train Loss: 0.00 | Acc: 97.38 | Time: 1.41\n",
            "[Epoch 5.65] Train Loss: 0.00 | Acc: 97.29 | Time: 1.46\n",
            "[Epoch 5.76] Train Loss: 0.00 | Acc: 97.96 | Time: 1.51\n",
            "[Epoch 5.86] Train Loss: 0.00 | Acc: 98.17 | Time: 1.45\n",
            "[Epoch 5.97] Train Loss: 0.00 | Acc: 98.46 | Time: 1.42\n",
            "[Epoch 6.00] Test Loss: 0.14 | Acc: 96.31\n",
            "[Epoch 6.00] Train Loss: 0.00 | Acc: 97.67 | Time: 1.40\n",
            "[Epoch 6.11] Train Loss: 0.00 | Acc: 98.25 | Time: 1.40\n",
            "[Epoch 6.22] Train Loss: 0.00 | Acc: 97.79 | Time: 1.76\n",
            "[Epoch 6.32] Train Loss: 0.00 | Acc: 97.46 | Time: 1.40\n",
            "[Epoch 6.43] Train Loss: 0.00 | Acc: 98.29 | Time: 1.40\n",
            "[Epoch 6.54] Train Loss: 0.00 | Acc: 97.92 | Time: 1.41\n",
            "[Epoch 6.65] Train Loss: 0.00 | Acc: 97.63 | Time: 1.40\n",
            "[Epoch 6.76] Train Loss: 0.00 | Acc: 98.29 | Time: 1.71\n",
            "[Epoch 6.86] Train Loss: 0.00 | Acc: 97.83 | Time: 1.41\n",
            "[Epoch 6.97] Train Loss: 0.00 | Acc: 98.12 | Time: 1.41\n",
            "[Epoch 7.00] Test Loss: 0.13 | Acc: 96.38\n",
            "[Epoch 7.00] Train Loss: 0.00 | Acc: 97.13 | Time: 1.42\n",
            "[Epoch 7.11] Train Loss: 0.00 | Acc: 97.21 | Time: 1.41\n",
            "[Epoch 7.22] Train Loss: 0.00 | Acc: 98.25 | Time: 1.41\n",
            "[Epoch 7.32] Train Loss: 0.00 | Acc: 97.38 | Time: 1.41\n",
            "[Epoch 7.43] Train Loss: 0.00 | Acc: 98.33 | Time: 1.44\n",
            "[Epoch 7.54] Train Loss: 0.00 | Acc: 98.38 | Time: 1.40\n",
            "[Epoch 7.65] Train Loss: 0.00 | Acc: 97.83 | Time: 1.43\n",
            "[Epoch 7.76] Train Loss: 0.00 | Acc: 98.46 | Time: 1.43\n",
            "[Epoch 7.86] Train Loss: 0.00 | Acc: 98.29 | Time: 1.41\n",
            "[Epoch 7.97] Train Loss: 0.00 | Acc: 98.17 | Time: 1.42\n",
            "[Epoch 8.00] Test Loss: 0.11 | Acc: 96.93\n",
            "[Epoch 8.00] Train Loss: 0.00 | Acc: 98.25 | Time: 1.43\n",
            "[Epoch 8.11] Train Loss: 0.00 | Acc: 98.08 | Time: 1.75\n",
            "[Epoch 8.22] Train Loss: 0.00 | Acc: 98.33 | Time: 1.42\n",
            "[Epoch 8.32] Train Loss: 0.00 | Acc: 98.17 | Time: 1.49\n",
            "[Epoch 8.43] Train Loss: 0.00 | Acc: 99.04 | Time: 1.43\n",
            "[Epoch 8.54] Train Loss: 0.00 | Acc: 98.33 | Time: 1.44\n",
            "[Epoch 8.65] Train Loss: 0.00 | Acc: 97.63 | Time: 1.73\n",
            "[Epoch 8.76] Train Loss: 0.00 | Acc: 98.67 | Time: 1.41\n",
            "[Epoch 8.86] Train Loss: 0.00 | Acc: 98.00 | Time: 1.40\n",
            "[Epoch 8.97] Train Loss: 0.00 | Acc: 98.92 | Time: 1.41\n",
            "[Epoch 9.00] Test Loss: 0.11 | Acc: 96.91\n",
            "[Epoch 9.00] Train Loss: 0.00 | Acc: 98.12 | Time: 1.41\n",
            "[Epoch 9.11] Train Loss: 0.00 | Acc: 98.29 | Time: 1.40\n",
            "[Epoch 9.22] Train Loss: 0.00 | Acc: 98.17 | Time: 1.41\n",
            "[Epoch 9.32] Train Loss: 0.00 | Acc: 96.21 | Time: 1.41\n",
            "[Epoch 9.43] Train Loss: 0.01 | Acc: 94.38 | Time: 1.48\n",
            "[Epoch 9.54] Train Loss: 0.01 | Acc: 94.83 | Time: 1.40\n",
            "[Epoch 9.65] Train Loss: 0.01 | Acc: 95.38 | Time: 1.43\n",
            "[Epoch 9.76] Train Loss: 0.00 | Acc: 96.54 | Time: 1.42\n",
            "[Epoch 9.86] Train Loss: 0.00 | Acc: 97.00 | Time: 1.44\n",
            "[Epoch 9.97] Train Loss: 0.00 | Acc: 96.83 | Time: 1.40\n",
            "[Epoch 10.00] Test Loss: 0.15 | Acc: 95.58\n"
          ]
        }
      ],
      "source": [
        "run_maml()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}