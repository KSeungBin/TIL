{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"BMovyigQE0fv"},"outputs":[],"source":["!pip install --quiet optuna"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fYVQYgS_E4LN"},"outputs":[],"source":["import os\n","\n","import optuna\n","from optuna.trial import TrialState\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.utils.data\n","from torchvision import datasets\n","from torchvision import transforms"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GVFGwCxrE8eC"},"outputs":[],"source":["# hyper parameter setting\n","DEVICE = torch.device(\"cuda\")\n","BATCHSIZE = 128\n","CLASSES = 10\n","DIR = os.getcwd()\n","EPOCHS = 10\n","N_TRAIN_EXAMPLES = BATCHSIZE * 30\n","N_VALID_EXAMPLES = BATCHSIZE * 10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o6IYniULFBBK"},"outputs":[],"source":["# 하나의 function 안에 모델/데이터/search space/objective function을 한꺼번에 선언할 수 없는 딥러닝 모델\n","# 따라서, search할 것이 있는 경우에만 인자로 trial을 넣어준다\n","\n","def define_model(trial):\n","    # We optimize the number of layers, hidden units(number of output nodes) and dropout ratio in each layer.\n","    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n","    layers = []\n","\n","    in_features = 28 * 28\n","    for i in range(n_layers):\n","        out_features = trial.suggest_int(\"n_units_l{}\".format(i), 4, 128)\n","        layers.append(nn.Linear(in_features, out_features))\n","        layers.append(nn.ReLU())\n","        p = trial.suggest_float(\"dropout_l{}\".format(i), 0.2, 0.5)\n","        layers.append(nn.Dropout(p))\n","\n","        in_features = out_features\n","    layers.append(nn.Linear(in_features, CLASSES))\n","    layers.append(nn.LogSoftmax(dim=1))\n","\n","    return nn.Sequential(*layers)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g2j6S_FWFDLO"},"outputs":[],"source":["def get_mnist():\n","    # Load FashionMNIST dataset.\n","    train_loader = torch.utils.data.DataLoader(\n","        datasets.FashionMNIST(DIR, train=True, download=True, transform=transforms.ToTensor()),\n","        batch_size=BATCHSIZE,\n","        shuffle=True,\n","    )\n","    valid_loader = torch.utils.data.DataLoader(\n","        datasets.FashionMNIST(DIR, train=False, transform=transforms.ToTensor()),\n","        batch_size=BATCHSIZE,\n","        shuffle=True,\n","    )\n","\n","    return train_loader, valid_loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"27mKuV3ZFEs4"},"outputs":[],"source":["def objective(trial):\n","\n","    # Generate the model.\n","    model = define_model(trial).to(DEVICE)\n","\n","    # Generate the optimizers.\n","    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n","    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n","    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n","\n","    # Get the FashionMNIST dataset.\n","    train_loader, valid_loader = get_mnist()\n","\n","    # Training of the model.\n","    for epoch in range(EPOCHS):\n","        model.train()\n","        for batch_idx, (data, target) in enumerate(train_loader):\n","            # Limiting training data for faster epochs.\n","            if batch_idx * BATCHSIZE >= N_TRAIN_EXAMPLES:\n","                break\n","\n","            data, target = data.view(data.size(0), -1).to(DEVICE), target.to(DEVICE)\n","\n","            optimizer.zero_grad()\n","            output = model(data)\n","            loss = F.nll_loss(output, target)\n","            loss.backward()\n","            optimizer.step()\n","\n","        # Validation of the model.\n","        model.eval()\n","        correct = 0\n","        with torch.no_grad():\n","            for batch_idx, (data, target) in enumerate(valid_loader):\n","                # Limiting validation data.\n","                if batch_idx * BATCHSIZE >= N_VALID_EXAMPLES:\n","                    break\n","                data, target = data.view(data.size(0), -1).to(DEVICE), target.to(DEVICE)\n","                output = model(data)\n","                # Get the index of the max log-probability.\n","                pred = output.argmax(dim=1, keepdim=True)\n","                correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","        accuracy = correct / min(len(valid_loader.dataset), N_VALID_EXAMPLES) # validation dataset -> correct -> accuracy(output)\n","\n","        trial.report(accuracy, epoch) # log\n","\n","        # Handle pruning based on the intermediate value.\n","        if trial.should_prune():\n","            raise optuna.exceptions.TrialPruned()\n","\n","    return accuracy # search의 목적(output)을 명확하게 해줘야 한다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wKsP5aBsFGy7"},"outputs":[],"source":["if __name__ == \"__main__\":\n","    study = optuna.create_study(direction=\"maximize\")\n","    study.optimize(objective, n_trials=100, timeout=600) # 딥러닝 모델이 너무 커질 수 있어 timeout 걸어주기\n","\n","    # 아래 두 줄로 activation pruning: trials의 뒤로 갈수록 uncertainty가 줄어들어 pruning이 많이 된다.\n","    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n","    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n","\n","    print(\"Study statistics: \")\n","    print(\"  Number of finished trials: \", len(study.trials))\n","    print(\"  Number of pruned trials: \", len(pruned_trials))\n","    print(\"  Number of complete trials: \", len(complete_trials))\n","\n","    print(\"Best trial:\")\n","    trial = study.best_trial\n","\n","    print(\"  Value: \", trial.value)\n","\n","    print(\"  Params: \")\n","    for key, value in trial.params.items():\n","        print(\"    {}: {}\".format(key, value))\n","\n","# pruning 기법: bayesian 이외에도 search한 후보가 내부 알고리즘에 의해 의미없는 후보라면 한번 더 걸러줌\n","# 따라서, 딥러닝 모델의 경우 의미 없는 search space를 가지치기해주는 prune을 활성화하기(10 epochs * 100 trials를 다 도는 것은 비효율적)"]},{"cell_type":"markdown","metadata":{},"source":["report의 정당성 부여:  Optuna로 ~ parameters에 대해서 ~ search space에서 n번의 trials를 통해 best parameter가 위와 같이 출력되었다. 따라서, 이들을 채택"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
