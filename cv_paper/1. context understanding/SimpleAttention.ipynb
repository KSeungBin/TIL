{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Attention Block\n",
    "- MultiHeadAttention\n",
    "- Encoder Layer\n",
    "- Pytorch Official Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('mps')\n",
    "# device = torch.device('cuda')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.Tensor(torch.randn(8,3)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1142, -0.1298,  0.6582],\n",
      "        [ 0.4922, -2.2240, -1.8335],\n",
      "        [-1.5907, -0.6409, -1.1080],\n",
      "        [ 1.1062, -1.4802, -0.3679],\n",
      "        [-0.1110,  1.1425,  0.4500],\n",
      "        [-0.8565, -0.7581,  0.9765],\n",
      "        [-0.3787, -0.4340,  2.5556],\n",
      "        [-0.3714, -0.8131,  0.1910]])\n"
     ]
    }
   ],
   "source": [
    "# Transformer, is, the, best, model, for, representation, learning\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 가능해야 하므로 parameter로 만들어준다\n",
    "W_Q = torch.nn.Parameter(torch.Tensor(torch.randn(3,2))).to(device)\n",
    "W_K = torch.nn.Parameter(torch.Tensor(torch.randn(3,2))).to(device)\n",
    "W_V = torch.nn.Parameter(torch.Tensor(torch.randn(3,2))).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Multiplication\n",
    "Q = torch.matmul(X, W_Q) # X.matmul(W_Q)\n",
    "K = torch.mm(X, W_K)\n",
    "V = X@W_V # broadcast 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K.shape torch.Size([8, 2])\n",
      "K.T.shape torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "# attention score : query와 key의 일치도 or 관련도 = Q @ K의 Transpose\n",
    "# self attention이므로 Q와 K가 동일하다. 따라서 attention_score의 size는 (8,8)\n",
    "attention_score = Q.matmul(K.T)\n",
    "print('K.shape: ', K.shape)\n",
    "print('K.T.shape: ', K.T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "d_k = K.shape[1] # feature dimension = 2\n",
    "print(d_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling\n",
    "attention_score = attention_score/(d_k**0.5)\n",
    "# attention_score = attention_score/(math.sqrt(d_k))\n",
    "# attention_score = attention_score/(torch.sqrt(d_k)) # d_k를 tensor로 변환한 후에 torch 연산할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -> 방향으로 0~1 사이의 확률값으로 변환\n",
    "attention_score = torch.softmax(attention_score, dim=1) # 함수로 구현\n",
    "# torch.nn.Softmax(attention_score, dim=1) # layer로 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_score[0,:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_score.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래처럼 query에 d_k를 나눠주면 16번의 연산만 하면 됨(<-> 위에서는 64번의 연산 발생) : 따라서, 아래처럼 사용된다\n",
    "# attention score = (8,8)의 weight matrix\n",
    "d_k = K.shape[1]\n",
    "attention_score = (Q/(d_k**0.5)).matmul(K.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "# weigted sum을 통해 (8,2) vector 생성\n",
    "# selt-attention block 완성\n",
    "Z = attention_score@V\n",
    "print(Z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('fastcampus')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d6f7e343c099eb3ca4a98d401039ba14fc86fe96c4e91f3dd0ae0aea7c169397"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
